\chapter{Methodology}
\label{chap:methodology}

This study employs a comparative analysis framework to measure the divergence between traditional information retrieval (Search) and generative information retrieval (GenAI). The methodology is designed to isolate the "grounding" behavior of LLMs by controlling for geographical context, query intent, and temporal variance.

\section{Data Collection Pipeline}
The dataset consists of 80 unique product recommendation queries focused on the SaaS and AI software domains (e.g., "Best AI video translators", "Top-rated transcription software"). These queries were selected based on high commercial intent and real-world usage patterns.

To ensure statistical significance and capture the stochastic nature of RAG systems, each prompt was executed in three independent runs, resulting in a total of 240 data points. The data collection pipeline integrated four distinct sources:

\begin{enumerate}
    \item \textbf{ChatGPT Data:} Full response generation including inline citations, "searched for" queries, and recommended entities.
    \item \textbf{Bing Search Data:} Retrieval of the Top 30 organic results for each query, plus a "Deep Hunt" retrieval of results up to Rank 150 to detect buried citations.
    \item \textbf{Gemini Data:} Extraction of \texttt{groundingMetadata}, specifically distinguishing between \texttt{groundingChunks} (the retrieved consideration set) and \texttt{groundingSupports} (the actually cited set).
    \item \textbf{Google SERP Data:} Retrieval of the Top 20 organic results via SerpApi to serve as a control group for the Gemini analysis.
\end{enumerate}

\section{The "Deep Hunt" Protocol}
A critical methodological innovation of this study is the "Deep Hunt" protocol. Initial pilot runs revealed that approximately 35\% of citations generated by ChatGPT were absent from the standard Top 30 Bing results. To determine whether these citations were truly "invisible" (absent from the index) or merely "buried" (ranking poorly), the retrieval depth was expanded to Rank 150. This revealed a phenomenon of "UI Suppression," where relevant content accessible to the API-based agent was effectively invisible to human users due to pagination limits and UI clutter.

\section{Measuring the "AI Search Filter"}
For the Google Gemini analysis, a specific metric was developed to quantify the model's filtering logic. By comparing the \texttt{groundingChunks} (content retrieved by the model) against the \texttt{groundingSupports} (content used by the model) and the external Google SERP (content ranked by the search engine), we calculate the \textbf{Rejection Rate}. This metric serves as a proxy for the model's internal quality filter, allowing us to analyze why high-ranking search results fail to achieve grounding status.

\section{Content Ingestion and Enrichment}
To analyze the "Content DNA" of cited versus rejected sources, a headless browser automation pipeline was developed to fetch the full HTML content of over 3,000 unique URLs. These documents were processed through a "Unified Judge" LLM pipeline to extract metadata such as:
\begin{itemize}
    \item \textbf{Tone:} (e.g., Promotional, Informational, Salesy)
    \item \textbf{Structure:} (Presence of tables, lists, schema markup)
    \item \textbf{Authority Signals:} (Clear authorship, citation density)
\end{itemize}
This enrichment step allows for a multivariate analysis of the factors driving "Extraction Efficiency" in Generative Engine Optimization.
